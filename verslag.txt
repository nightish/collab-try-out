ChatGPT is een lid van de generatieve voorgetrainde transformator (GPT) familie van taalmodellen. Het werd verfijnd (een benadering van transfer learning) ten  opzichte van een verbeterde versie van OpenAI's GPT-3 bekend als "GPT 3.5". Het fine-tuningproces maakte gebruik van zowel supervised learning als reinforcement learning in een proces dat reinforcement learning from human feedback (RLHF)   wordt genoemd. Beide benaderingen gebruikten menselijke trainers om de prestaties van het model te verbeteren. 

In het geval van supervised learning werd het model voorzien van gesprekken waarin de trainers aan beide kanten speelden: de gebruiker en de AI-assistent. In de reinforcement learning-stap rangschikten menselijke trainers eerst reacties die het model in een eerder gesprek had gemaakt.

Deze ranglijsten werden gebruikt om 'beloningsmodellen' te maken waarop het     model verder werd afgestemd met behulp van verschillende iteraties van Proximale Beleidsoptimalisatie (PPO) Proximale beleidsoptimalisatie-algoritmen bieden een kosteneffectief voordeel voor algoritmen voor het vertrouwen in regiobeleidsoptimalisatie; ze ontkrachten veel van de rekensomrijke bewerkingen met snellere prestaties. De modellen werden getraind in samenwerking met Microsoft op hun Azure supercomputing infrastructuur, met behulp van Nvidia GPU's, "supercomputer ontwikkeld voor OpenAI is een enkel systeem met meer dan 285.000 CPU-kernen, 10.000 GPU's en 400 gigabit per seconde netwerkconnectiviteit voor elke GPU-server".

Daarnaast blijft OpenAI gegevens van ChatGPT-gebruikers verzamelen die kunnen worden gebruikt om ChatGPT verder te trainen en te verfijnen. Gebruikers kunnen reacties die ze ontvangen van ChatGPT upvoten of downvoten en een tekstveld invullen met aanvullende feedback.
